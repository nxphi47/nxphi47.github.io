---
title: "Data Diversification: An Elegant Strategy For Neural Machine Translation"
excerpt: "A simple way to boost many NMT tasks by using multiple backward and forward models."
collection: publications
date: 2020-09-25
venue: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada
paperurl: 'https://arxiv.org/abs/1911.01986'
thumbnail: /images/publications/data-diversification.png
citation: 'Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, & Ai Ti Aw (2019). Data Diversification: An Elegant Strategy For Neural Machine Translation. In the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada, 2020.'
---


<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>



<h2>Abstract</h2>
<p>
    We introduce Data Diversification: a simple strategy to boost neural machine translation (NMT) performance. It
    diversifies the training data by using the predictions of multiple forward and backward models and then merging
    them with the original dataset on which the final NMT model is trained. Our method is applicable to all NMT models.
    It does not require extra monolingual data like back-translation, nor does it add more computations and parameters
    like ensembles of models. In the experiments, our method achieves state-of-the-art BLEU score of 30.7 & 43.7 in the
    WMT'14 English-German & English-French tasks. It also substantially improves on 8 other translation tasks: 4 IWSLT
    tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala).
    We demonstrate that our method is more effective than knowledge distillation and dual learning, it exhibits strong
    correlation with ensembles of models, and it trades perplexity off for better BLEU score. We have released our
    source code at this <a href="https://github.com/nxphi47/data_diversification" target="_blank">URL</a>.
</p>

<h2>Summary</h2>

<p>
    Data Diversification strategy trains the models in $N$ rounds. In the first round, we train $k$ forward models
    $(M^1_{S \rightarrow T,1},...,M^k_{S \rightarrow T,1})$ and $k$ backward models
    $(M^1_{T \rightarrow S,1},..,M^k_{T \rightarrow S,1})$, where $k$ denotes a diversification factor.
    Then, we use the forward models to translate the source-side corpus $S$ of the original data to generate synthetic
    training data. In other words, we obtain multiple synthetic target-side corpora as
    $(M^1_{S \rightarrow T,1}(S),...,M^k_{S \rightarrow T,1}(S))$. Likewise, the backward models are used to translate
    the target-side original corpus $T$ to synthetic source-side corpora as $(M^1_{T \rightarrow S,1}(T),...,M^k_{T \rightarrow S,1}(T))$.
    After that, we augment the original data with the newly generated synthetic data, which is summed up to the new
    round-1 data $\mathbf{D}_1$ as follows:
    \begin{equation}
        \mathbf{D}_1 = (S,T) \bigcup \cup_{i=1}^k (S, M^i_{S \rightarrow T,1}(S)) \bigcup \cup_{i=1}^k (M^i_{T \rightarrow S,1}(T), T)
    \end{equation}
    After that, process continues for round 2. The following algorithm summarize the method.
</p>


<img src='/images/publications/data-diversification.png'>

<h3>Experiments</h3>

<p>
    We compare our method against the state of the art <a href="https://arxiv.org/abs/1806.00187" target="_blank">Scale Transformer</a>
    and <a href="https://openreview.net/forum?id=SkVhlh09tX" target="_blank">Dynamic Convolution</a> in the WMT'14 English-German
    and English-French translation tasks. As shown in the table below, our method outperform the baselines around 1 BLEU.
    In comparison with other supporting techniques, such as
    <a href="https://arxiv.org/abs/1606.07947" target="_blank">Knowledge Distillation</a> or
    <a href="https://openreview.net/forum?id=HyGhN2A5tm" target="_blank">Multi-agent Dual Learning</a>, our method also
    shows superior performance.
    <br><br>
    Plus, we tested our method on smaller scale IWSLT'14 English-German and IWSLT'13 English-French tasks. Our method also shows
    significant improvements.
</p>
<img src='/images/publications/data-diversification/experiment-wmt-iwslt.png'>

<p>
    In addition, our method improves the translation performances of low-resource languages, such as Nepali and Sinhala.
</p>
<img src='/images/publications/data-diversification/experiment-low-resource.png'>


<h3>Understanding Data Diversification</h3>

<p>
    We conducted a series of experiments and analyses to find out how and why data diversification works well despite
    being very simple. In the following, we summarize our findings and conclusions with further experiments.
</p>

<ul>
    <li>
        Data diversification exhibits a strong correlation with ensemble of models. The table below show the performance
        comparison, while we explain further mathematically in the paper.
    </li>
    <li>
        Forward translation is crucial to the performance gain, which is in contrast to the common belief that back-translation
        is more important.
    </li>
</ul>

<img src='/images/publications/data-diversification/experiment-ensemble-fw-bw.png'>

<ul>
    <li>
        Data diversification sacrifices perplexity for better BLEU score. As shown in the table below, our method increases
        perplexity (PPL) but also improves the BLEU performance.
    </li>
    <li>
        Randomization of initial parameters of multiple models are vital to the diversity of synthetic, hence the final performance.
    </li>
</ul>
<img src='/images/publications/data-diversification/experiment-ppl-randomized.png'>

<h3>Complementary to Back-translation in Semi-supervised setup</h3>
<p>
    We found that our method further improves back-translation.
</p>
<img src='/images/publications/data-diversification/experiment-bt.png'>


<h3>Ablation Study</h3>
<p>
    Our ablation study shows that increasing the number of rounds $N$ does not significant improvement. However, increasing
    factor $k$ drastically improves the performance but we observe diminishing return.
</p>
<img src='/images/publications/data-diversification/experiment-ablation.png'>



