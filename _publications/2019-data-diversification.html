---
title: "Data Diversification: An Elegant Strategy For Neural Machine Translation"
excerpt: "A simple way to boost many NMT tasks by using multiple backward and forward models."
collection: publications
date: 2020-01-20
venue: arXiv preprint
paperurl: 'https://arxiv.org/abs/1911.01986'
thumbnail: /images/publications/data-diversification.png
citation: 'Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, & Ai Ti Aw (2019). Data Diversification: An Elegant Strategy For Neural Machine Translation arXiv preprint arXiv:1911.01986.'
---


<h2>Abstract</h2>
<p>
    We introduce Data Diversification: a simple strategy to boost neural machine translation (NMT) performance. It
    diversifies the training data by using the predictions of multiple forward and backward models and then merging
    them with the original dataset on which the final NMT model is trained. Our method is applicable to all NMT models.
    It does not require extra monolingual data like back-translation, nor does it add more computations and parameters
    like ensembles of models. In the experiments, our method achieves state-of-the-art BLEU score of 30.7 & 43.7 in the
    WMT'14 English-German & English-French tasks. It also substantially improves on 8 other translation tasks: 4 IWSLT
    tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala).
    We demonstrate that our method is more effective than knowledge distillation and dual learning, it exhibits strong
    correlation with ensembles of models, and it trades perplexity off for better BLEU score. We have released our
    source code at this <a href="https://github.com/nxphi47/data_diversification" target="_blank">URL</a>.
</p>

<h2>Summary</h2>

<img src='/images/publications/data-diversification.png'>




