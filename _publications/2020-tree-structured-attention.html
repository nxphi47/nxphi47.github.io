---
title: "Tree-structured Attention with Hierarchical Accumulation"
excerpt: A novel attention mechanism that aggregates hierarchical structures to encode constituency trees for downstream tasks.
collection: publications
date: 2020-04-20
venue: International Conference on Learning Representations (ICLR)
paperurl: 'https://arxiv.org/abs/2002.08046'
thumbnail: /images/publications/tree-structured-attention.png
citation: 'Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, & Richard Socher (2020). Tree-Structured Attention with Hierarchical Accumulation. In International Conference on Learning Representations.'
---

<h2>Abstract</h2>
<p>
    Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural
    language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the
    Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM,
    while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper,
    we attempt to bridge this gap with "Hierarchical Accumulation" to encode parse tree structures into self-attention
    at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14
    English-German translation task. It also yields improvements over Transformer and Tree-LSTM on three text
    classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage,
    and that our model prefers phrase-level attentions over token-level attentions.
</p>

<h2>Summary</h2>

<img src='/images/publications/tree-structured-attention.png'>

